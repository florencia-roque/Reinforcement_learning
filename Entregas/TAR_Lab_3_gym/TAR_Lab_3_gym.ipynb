{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f1674d",
   "metadata": {},
   "source": [
    "# TAR: Taller de Aprendizaje por Refuerzo 2025\n",
    "## Laboratorio 4: Introdución a Gymnasium\n",
    "### ¿Qué es Gymnasium?\n",
    "\n",
    "**Gymnasium** es una librería ampliamente utilizada en el campo del aprendizaje por refuerzos. Proporciona una colección de entornos estandarizados, como simulaciones de juegos, tareas físicas y otros desafíos, que nos permiten evaluar y entrenar agentes en una variedad de escenarios. La simplicidad y flexibilidad de Gymnasium lo convierten en una herramienta ampliamente utilizada. Documentación https://gymnasium.farama.org/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5301f9cd",
   "metadata": {},
   "source": [
    "*Ejecutar esta celda solo la primera vez (si estan usando un entorno local) para descargar e instalar los paquetes necesarios. Si ejecutan el notebook en colab tendran que ejecutarla cada vez que reinicien el kernel*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1153d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install swig cmake\n",
    "!pip install -r https://raw.githubusercontent.com/huggingface/deep-rl-class/main/notebooks/unit1/requirements-unit1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18494422",
   "metadata": {},
   "source": [
    "#### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448a69c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ffcdb2",
   "metadata": {},
   "source": [
    "## Ejercicio 1. Frozen lake\n",
    "\n",
    "En este ejercicio nos enfrentamos al problema de [Frozen Lake](https://gymnasium.farama.org/environments/toy_text/frozen_lake/), donde el agente aprenderá a navegar desde el **estado inicial (S)** hasta una **meta (G)**, evadiendo los **agujeros (H)** y caminando solamente sobre los bloques de **hielo (F)**.\n",
    "\n",
    "Podemos tener dos tamaños de entorno:\n",
    "- map_name=\"4x4\": una versión de cuadrícula 4x4\n",
    "- map_name=\"8x8\": una versión de cuadrícula 8x8\n",
    "\n",
    "\n",
    "El entorno tiene dos modos:\n",
    "- is_slippery=False: El agente siempre se mueve en la dirección deseada  (determinístico).\n",
    "- is_slippery=True: El agente puede no moverse siempre en la dirección deseada debido al hielo resbaladizo (estocástico).\n",
    "\n",
    "Trabajaremos con la versión 8x8, y determinística.\n",
    "\n",
    "### Implementaremos un agente basado en Q-learning\n",
    "¿Que tipo de metodo es este?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c836ceb",
   "metadata": {},
   "source": [
    "##### RESPUESTA -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73be8e83",
   "metadata": {},
   "source": [
    "#### 1.1 Definir el ambiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d71c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Definir el ambiente `FrozenLake-v1` y renderizarlo en modo rgb_array\n",
    "#env =\n",
    "\n",
    "# Reiniciar el entorno y renderizar la imagen inicial\n",
    "env.reset()\n",
    "image = env.render()\n",
    "\n",
    "# Mostrar la imagen del entorno con matplotlib\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "#TODO Imprimir la cantidad de posibles estados y la cantidad de acciones posibles\n",
    "state_space = #....\n",
    "print(\"Hay \", state_space, \" estados posibles\")\n",
    "\n",
    "action_space = #....\n",
    "print(\"Hay \", action_space, \" acciones posibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d351bdcc",
   "metadata": {},
   "source": [
    "Cada uno de los estados es representado por un entero, de izquierda a derecha y de arriba a abajo. El estado inicial es $S = **$ y el estado final $G = **$.\n",
    "\n",
    "El espacio de acciones es discreto, con 4 acciones disponibles:\n",
    "- 0: ...\n",
    "- 1: ...\n",
    "- 2: ...\n",
    "- 3: ...\n",
    "\n",
    "Función de recompensa:\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1409dd6",
   "metadata": {},
   "source": [
    "### Definición de las políticas de decisión\n",
    "\n",
    "Como Q-learning es un algoritmo off-policy, se utiliza una política de decisión para actuar y otra diferente para actualizar la tabla Q.\n",
    "Se utilizarán:\n",
    "- Política greedy (política de actualización)\n",
    "- Política epsilon-greedy (política de actuación)\n",
    "\n",
    "#### 1.2 Crear una función que implemente la política greedy. Las entradas son la tabla Q y el estado actual, y la salida es la acción a tomar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffb8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO crear una funcion que implemente la politica greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5133e70c",
   "metadata": {},
   "source": [
    "#### 1.3 Crear otra función que implemente la política epsilon-greedy. \n",
    "Además de los parámetros de entrada de la función anterior, agregar $ϵ$, que es la variable que controla la relación exploración/explotación a la hora de decidir que acción tomar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO crear una funcion que implemente la politica epsilon greedy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e69f22",
   "metadata": {},
   "source": [
    "Una vez creadas las funciones anteriores, solamente falta crear la función de entrenamiento. La misma debe seguir el siguiente pseudo-código:\n",
    "\n",
    "```\n",
    "Para cada episodio en el total de episodios de entrenamiento:\n",
    "\n",
    "Reducir epsilon (ya que necesitamos cada vez menos exploración)\n",
    "Reiniciar el entorno\n",
    "\n",
    "  Para cada paso en el máximo número de timesteps:\n",
    "    Elegir la acción At utilizando la política epsilon-greedy\n",
    "    Realizar la acción (a) y observar el nuevo estado (s') y la recompensa (r)\n",
    "    Actualizar el valor de Q(s,a) usando la ecuación de Bellman\n",
    "    Si el episodio ha terminado, finalizarlo\n",
    "\n",
    "```\n",
    "\n",
    "**Actualización de la Tabla Q**\n",
    "  \n",
    "La función de valor Q se actualiza utilizando la ecuación de Bellman:  \n",
    "  \n",
    "$$ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ R + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right] $$\n",
    "  \n",
    "donde:  \n",
    "- $ \\alpha $ es la tasa de aprendizaje.  \n",
    "- $ \\gamma $ es el factor de descuento.  \n",
    "- $ R $ es la recompensa recibida.  \n",
    "- $ s' $ es el nuevo estado.  \n",
    "- $ a' $ es la acción que maximiza el valor Q en el nuevo estado.\n",
    "\n",
    "**Observación**: Es importante que la variable `epsilon` varie entre un valor máximo y mínimo, y que vaya decayendo de manera exponencial según el `decay_rate` y el número de episodio en el que se esté.\n",
    "\n",
    "**Observación 2**: Almacenar la ganacia acumulada para cada episodio para poder graficar la evolución de entrenamiento\n",
    "\n",
    "#### 1.4 Crear la función de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, gamma, learning_rate, Qtable):\n",
    "  rewards = []\n",
    "  #....\n",
    "  return Qtable, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c314e2",
   "metadata": {},
   "source": [
    "#### 1.5 Definir los hiperparámetros, inicializar la tabla Q llena de ceros y entrenar el agente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c53146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de entrenamiento\n",
    "n_training_episodes = #....  # Total de episodios de entrenamiento\n",
    "learning_rate = #....          # Tasa de aprendizaje\n",
    "\n",
    "# Parámetros del entorno\n",
    "max_steps = #....              # Máximo de pasos por episodio\n",
    "gamma = #....                 # Tasa de descuento\n",
    "\n",
    "# Parámetros de exploración\n",
    "max_epsilon = #....           # Probabilidad de exploración al inicio\n",
    "min_epsilon = #....           # Probabilidad mínima de exploración\n",
    "decay_rate = #....          # Tasa de decaimiento de epsilon\n",
    "\n",
    "# inicializar la tabla Q con ceros\n",
    "Qtable_frozenlake = #...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b71d830",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qtable_frozenlake, rewards = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, gamma, learning_rate, Qtable_frozenlake)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1b840",
   "metadata": {},
   "source": [
    "#### 1.6 El siguente código grafica el promedio de recompensas por cada bloque de 1000 episodios. Observar y explicar su comportamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d0b5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el promedio de recompensas por cada bloque de 1000 episodios\n",
    "reward_per_thousand_episodes = np.split(np.array(rewards), n_training_episodes/1000)\n",
    "count = 1000\n",
    "avg_rewards = []\n",
    "\n",
    "for r in reward_per_thousand_episodes:\n",
    "    avg_rewards.append(sum(r/1000))\n",
    "    count += 1000\n",
    "\n",
    "# Visualizamos el promedio de recompensas\n",
    "plt.plot(avg_rewards)\n",
    "plt.xlabel('Bloques de 1000 episodios')\n",
    "plt.ylabel('Promedio de Recompensas')\n",
    "plt.title('Promedio de Recompensas por Episodio durante el Entrenamiento')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb128337",
   "metadata": {},
   "source": [
    "#### 1.7 Simular un juego y obtener la ganancia (reward) total acumulada. Ver si el agente logró llegar a la meta (si el roward total es positivo).\n",
    "\n",
    "En caso contrario variar los hiperparámetros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc990cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_reward = 0\n",
    "\n",
    "# Jugar una vez con la tabla Q entrenada y obtener la ganancia acumulada\n",
    "#....\n",
    "\n",
    "if total_reward > 0:\n",
    "  print('Llgegaste a la meta!')\n",
    "print('total_reward ', total_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26b3ab9",
   "metadata": {},
   "source": [
    "#### 1.8 Qué impacto tiene en la solución obtenida los diferentes hiperparámetros? Qué sucede al variar el `decay_rate` y `max_epsilon` y `min_epsilon`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f558c2",
   "metadata": {},
   "source": [
    "##### RESPUESTA -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6040ea",
   "metadata": {},
   "source": [
    "#### 1.9 Si se quisiera un agente que tome el camino más corto, ¿qué solución/es se le ocurre? Implementarla/s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec1870c",
   "metadata": {},
   "source": [
    "##### RESPUESTA -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92239993",
   "metadata": {},
   "source": [
    "La siguente función toma la tabla Q entrenada y simula un juego, guardando los frames y generando un video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ba1176",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "def generate_frames(q_table, env, max_steps=100):\n",
    "    \"\"\"Genera una lista de frames para crear la animación\"\"\"\n",
    "    state, _ = env.reset()\n",
    "    frames = [env.render()]  # Render del estado inicial\n",
    "    for step in range(max_steps):\n",
    "        action = greedy_policy(q_table, state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        frames.append(env.render())  # Guardar el nuevo estado como frame\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    return frames\n",
    "\n",
    "def save_video(frames, filename='q_learning_frozenlake.mp4'):\n",
    "    \"\"\"Guardar el video de los frames en un archivo .mp4\"\"\"\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(fig, animate, frames=len(frames), interval=300)\n",
    "\n",
    "    # Guardar animación\n",
    "    anim.save(filename, writer='ffmpeg', fps=5)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8bdf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar los frames a partir de la tabla Q\n",
    "frames = generate_frames(Qtable_frozenlake, env)\n",
    "\n",
    "# Guardar el video\n",
    "save_video(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a797da3",
   "metadata": {},
   "source": [
    "## Ejercicio 2. Ta-te-ti usando Q-learning\n",
    "\n",
    "En esta sección, vamos a entrenar un agente de aprendizaje por refuerzo utilizando el algoritmo de **Q-Learning** para jugar al juego de **Ta-Te-Ti**. El objetivo es que el agente aprenda a tomar decisiones óptimas en cada estado del juego para maximizar su recompensa y ganar la mayor cantidad de partidas posible.\n",
    "\n",
    "En el archivo ta-te-ti_env.py se define el entorno del **Ta-Te-Ti** que utilizaremos para entrenar nuestro agente de Q-Learning. Buscamos crear un entorno compatible con las interfaces de Gym, lo que significa que debe incluir métodos como `reset`, `render`, `step`, así como propiedades como `observation_space` y `action_space`. Es importante leer y entender la definición del ambiente antes de continuar.\n",
    "\n",
    "El entorno de **Ta-Te-Ti** consta de un tablero de $3\\times3$, para el cual se utiliza un vector de $9$ valores para definir su estado. Este vector se rellena con valores $0$ para los lugares disponibles, con $1$ para  indicar las posiciones donde juega el agente y con $-1$ para indicar las posiciones del oponente.\n",
    "El environment permite recibir como parámetros de inicialización los reward para el caso donde el agente gana, pierde o empata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96f34ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ta_te_ti_env import TaTeTiEnv, play_against_agent\n",
    "\n",
    "env = TaTeTiEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527abee8",
   "metadata": {},
   "source": [
    "#### 2.1. Observar la cantidad de posibles estados y posibles acciones a tomar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a46c9947",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_space = #....\n",
    "# print(\"Hay \", state_space, \" estados posibles\")\n",
    "\n",
    "# action_space = #....\n",
    "# print(\"Hay \", action_space, \" acciones posibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95684c42",
   "metadata": {},
   "source": [
    "#### 2.2. Definir los hiperparámetros y entrenar el agente utilizando las mismas funciones creadas para la parte 2. Variar los diferentes hiperparámetros y diferentes rewards para mejorar la tabla Q obtenida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871910fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "win_reward = #....\n",
    "draw_reward = #....\n",
    "loss_reward = #....\n",
    "env = TaTeTiEnv(win_reward, draw_reward, loss_reward) # Se define el entorno\n",
    "\n",
    "# Parámetros de entrenamiento\n",
    "n_training_episodes = #....  # Total de episodios de entrenamiento\n",
    "learning_rate = 0.7          # Tasa de aprendizaje\n",
    "\n",
    "# Parámetros del entorno\n",
    "max_steps = 9               # Máximo de pasos por episodio\n",
    "gamma = #....                # Tasa de descuento\n",
    "eval_seed = []               # La semilla de evaluación del entorno\n",
    "\n",
    "# Parámetros de exploración\n",
    "max_epsilon = #....            # Probabilidad de exploración al inicio\n",
    "min_epsilon = #....           # Probabilidad mínima de exploración\n",
    "decay_rate = #....         # Tasa de decaimiento de epsilon\n",
    "\n",
    "Qtable = np.zeros((env.observation_space.n, env.action_space.n)) # Inicializamos la tabla Q\n",
    "Qtable, rewards = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, learning_rate, gamma, Qtable) # Entrenamos el agente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433f62ad",
   "metadata": {},
   "source": [
    "Se observa a continuación la evolución de la recompensa a medida que se avanza el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddf774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculamos el promedio de recompensas por cada bloque de 1000 episodios\n",
    "reward_per_thousand_episodes = np.split(np.array(rewards), n_training_episodes/1000)\n",
    "count = 1000\n",
    "avg_rewards = []\n",
    "\n",
    "for r in reward_per_thousand_episodes:\n",
    "    avg_rewards.append(sum(r/1000))\n",
    "    count += 1000\n",
    "\n",
    "# Visualizamos el promedio de recompensas\n",
    "plt.plot(avg_rewards)\n",
    "plt.xlabel('Bloques de 1000 episodios')\n",
    "plt.ylabel('Promedio de Recompensas')\n",
    "plt.title('Promedio de Recompensas por Episodio durante el Entrenamiento')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00961845",
   "metadata": {},
   "source": [
    "#### 2.3 Jugar contra el agente, y observar donde y cuando falla. Ajustar hiperparámetros y rewards para mejorar su rendimiento\n",
    "\n",
    "En particular, experimentar con distintos valores de recompensa. Es interesante visualizar que pasa para los casos donde la recompensa negativa (cuando el agente pierde) es más fuerte que para cuando gana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75a7492",
   "metadata": {},
   "outputs": [],
   "source": [
    "play_against_agent(Qtable, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
